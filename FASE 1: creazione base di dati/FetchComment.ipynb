{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05628917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"C:\\\\Users\\\\Tiziano Pacifico\\\\Desktop\\\\CodeTesi\")\n",
    "import os\n",
    "os.chdir('C:\\\\Users\\\\Tiziano Pacifico\\\\Desktop\\\\TESI\\\\RedditDS')\n",
    "import credential\n",
    "import praw\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd29931",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "Partendo dai singoli blocchi di submissions ottenuti dalla scissione del dataframe originale, abbiamo ottenuto una uguale serie di dataframes con i commenti relativi ad ogni blocco di submissions.\n",
    "Abbiamo iterato progressivamente sui singoli file json e per ogni json abbiamo iterato sulle singole submissions. PEr ogni submission abbiamo estrato la foresta dei commenti senza preoccuparti della posizione gerarchiaca degli stessi nella foresta in quanto la nostra analisi si concetrerà prevalentemente sul testo indipendentemente dall'ordine in cui esso compare. \n",
    "L'estrazione è stata però fatta in questo caso attraverso delle liste. Ogni lista corrisponde alla serie che verrà inserita nella rispettiva colonna del dataframe finale. Il tempo di esecuzione dello script si è rivelato molto più basso delle attese permettendoci di estrarre la quasi totalità dei commenti in meno di una giornata.\n",
    "Anche per i commenti alcune verifiche a campione hanno riportato una miss rate inferiore al 5% un valore più che accettabile al fine della nostra analisi.\n",
    "Qualche eccezione è stata sollevata rispetto agli autori dei commenti, alcuni di essi infatti non hanno resituito nessun autore. Il motivo di questa assenza è ancora da indagare ma anche in questo caso il miss rate è inferiore in media al 5% per cui possiamo ritenere il notro dataset come valido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95d81dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id = credential.CLIENT_ID,\n",
    "                     client_secret = credential.CLIENT_SECRET,\n",
    "                     user_agent = 'fetching comments: u/Ardenzio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "897fbeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['id','body','author','created_utc','sub_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9a7f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 51\n",
    "end = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e31ffc36",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "for j in range(start,end):\n",
    "    print(j)\n",
    "    file_path = f'splitted_df_sub\\\\splitted_df_sub{j}.json'\n",
    "    df_submission = pd.read_json(file_path, lines = True)\n",
    "    id_list = list(df_submission['id'])\n",
    "    df_comments = pd.DataFrame(columns=cols)\n",
    "    df_comments = df_comments.astype({'created_utc': np.int64})\n",
    "        \n",
    "    comments_id = []\n",
    "    comments_body = []\n",
    "    comments_author = []\n",
    "    comments_utc = []\n",
    "    comments_sub_id = []\n",
    "        \n",
    "    for id in id_list:\n",
    "        submission = reddit.submission(id)\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        for comment in submission.comments.list():\n",
    "            comments_id.append(comment.id)\n",
    "            comments_body.append(comment.body)\n",
    "\n",
    "            if comment.author:\n",
    "                if comment.author.name:\n",
    "                    comments_author.append(comment.author.name)\n",
    "                else:\n",
    "                    comments_author.append(None)\n",
    "            else:\n",
    "                comments_author.append(None)\n",
    "\n",
    "            comments_utc.append(comment.created_utc) \n",
    "            comments_sub_id.append(id)\n",
    "        \n",
    "    df_comments[cols[0]] = comments_id\n",
    "    df_comments[cols[1]] = comments_body\n",
    "    df_comments[cols[2]] = comments_author\n",
    "    df_comments[cols[3]] = comments_utc\n",
    "    df_comments[cols[4]] = comments_sub_id\n",
    "        \n",
    "    save_file = f'splitted_df_com\\\\splitted_df_comm{j}.json'\n",
    "        \n",
    "    df_comments.to_json(save_file, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4100a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
